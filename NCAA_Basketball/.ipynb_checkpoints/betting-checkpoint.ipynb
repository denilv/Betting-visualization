{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LSTM for predicting winners with pick and vegas - normal algorithm\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "from IPython.display import SVG\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.utils.visualize_util import plot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import SVG\n",
    "# from keras.utils.visualize_util import model_to_dot\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams = []\n",
    "def sigmoid(c, x):\n",
    "    return 1 / (1 + math.exp(-c * x))\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def normalize_team(dataset):\n",
    "    team_matrix = []\n",
    "    team_names = []\n",
    "    for i in range(0, len(dataset)):\n",
    "        time = dataset[i, 3]\n",
    "        team = mapping(dataset[i, 4])\n",
    "        opponent_team = mapping(dataset[i, 5])\n",
    "        teamname = dataset[i, 4]\n",
    "        opponteamname = dataset[i, 5]\n",
    "        point = dataset[i, 6]\n",
    "        opponent_point = dataset[i, 7]\n",
    "        spread = dataset[i, 9]\n",
    "        point_diff = numpy.subtract(numpy.int_(point), numpy.int_(opponent_point))\n",
    "        diff = numpy.add(point_diff, spread)\n",
    "        consensus_sp = 1\n",
    "        if dataset[i, 11] >= 50:\n",
    "            if diff > 0:\n",
    "                consensus_sp = 1\n",
    "            else:\n",
    "                consensus_sp = 0\n",
    "        else:\n",
    "            if diff > 0:\n",
    "                consensus_sp = 0\n",
    "            else:\n",
    "                consensus_sp = 1\n",
    "        # opponent_pick = 0\n",
    "        # if i < len(dataset) - 1:\n",
    "        #   opponent_pick = dataset[i + 1, 10]\n",
    "        # else:\n",
    "        #   opponent_pick = 0\n",
    "        team_matrix.append(numpy.concatenate((team, opponent_team, [time.split(':', 1)[0]])))\n",
    "        team_names.append([teamname, opponteamname, consensus_sp, spread, point, opponent_point])\n",
    "    dataset = numpy.concatenate((dataset, team_matrix), axis=1)\n",
    "    dataset = numpy.delete(dataset, [1, 3, 4, 5], axis = 1)\n",
    "    dataset = dataset[::2]\n",
    "    team_names = team_names[::2]\n",
    "    return numpy.array(dataset), numpy.array(team_names)\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(data):\n",
    "    dataX, dataY, team_matrix = [], [], []\n",
    "    dataX = numpy.delete(data, [1, 2, 3], axis = 1)\n",
    "    for i in range(0, len(data)):\n",
    "        dataY.append([data[i, 3]])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# standardize name of teams\n",
    "def mapping(name):\n",
    "    # training dataset generation\n",
    "    # teams = [ 'DAL', 'WAS', 'POR', 'NYK', 'SAC', 'DET', 'ATL', 'NOP', 'LAC', 'DEN', 'PHI', 'LAL', 'CLE', 'CHI', 'IND', 'TOR', 'ORL', 'MIA', 'MEM', 'BOS', 'MIL', 'BKN', 'OKC', 'HOU', 'PHX', 'MIN', 'UTA', 'GSW', 'SAS', 'CHA', 'EAST', 'WEST', 'WALLSTAR', 'USALLSTAR' ]\n",
    "    normalized_values = numpy.eye(len(teams))\n",
    "    index = teams.index(name)\n",
    "    return normalized_values[index]\n",
    "\n",
    "# get Percentage of weekly prediction\n",
    "def getPercentage(data):\n",
    "    percent1 = 0\n",
    "    percent2 = 0\n",
    "    percent3 = 0\n",
    "    result_winloss_array = []\n",
    "    result_winloss_array2 = []\n",
    "    data = numpy.array(data)\n",
    "    for i in range(0, len(data)):\n",
    "        point_diff1 = numpy.subtract(numpy.int_(data[i, 4]), numpy.int_(data[i, 5]))\n",
    "        diff1 = numpy.sign(numpy.add(point_diff1, numpy.float_(data[i, 3])))\n",
    "        point_diff2 = numpy.int_(data[i, 6])\n",
    "        diff2 = numpy.sign(numpy.add(point_diff2, numpy.float_(data[i, 3])))\n",
    "\n",
    "        if diff1 == diff2:\n",
    "            percent1 = percent1 + 1\n",
    "            result_winloss_array.append(1)\n",
    "        else:\n",
    "            result_winloss_array.append(0)\n",
    "\n",
    "        if numpy.sign(point_diff1) == numpy.sign(point_diff2):\n",
    "            percent3 = percent3 + 1\n",
    "            result_winloss_array2.append(1)\n",
    "        else:\n",
    "            result_winloss_array2.append(0)\n",
    "\n",
    "        if numpy.int_(data[i, 2]) == 1:\n",
    "            percent2 = percent2 + 1\n",
    "    value1 = (percent1 / float(len(data))) * 100\n",
    "    value2 = (percent2 / float(len(data))) * 100\n",
    "    value3 = (percent3 / float(len(data))) * 100\n",
    "    return numpy.around(value1, decimals = 2), numpy.around(value2, decimals = 2), numpy.around(value3, decimals = 2), result_winloss_array, result_winloss_array2\n",
    "\n",
    "# get lengths of both training and testing data from csv\n",
    "def getLength(dataset, date):\n",
    "    num_date = numpy.searchsorted(dataset[:,1], date)\n",
    "    trainLength = num_date\n",
    "    # trainLength = num_date - 1\n",
    "    num_date = [i for i,val in enumerate(dataset[:,1]) if val == date]\n",
    "    testLength = num_date[-1] + 1\n",
    "    # testLength = num_date[-1]\n",
    "    return trainLength, testLength\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getConfidence(testTeams, d_set, testPredict, select_date):\n",
    "    win_loss = []\n",
    "    d_set = d_set[::2]\n",
    "    home_margin = 0\n",
    "    num_date = numpy.searchsorted(d_set[:,2], select_date)\n",
    "    trainLength = num_date\n",
    "    num_date = [i for i,val in enumerate(d_set[:,2]) if val == select_date]\n",
    "    testLength = num_date[-1]\n",
    "    k = 0\n",
    "    for i in range(trainLength, testLength + 1):\n",
    "        team1_pos, team1_nav, team2_pos, team2_nav = 0, 0, 0, 0\n",
    "        for j in range(trainLength - 2000, trainLength):\n",
    "            if d_set[j][4] == d_set[i][4]:\n",
    "                diff = numpy.subtract(numpy.int_(d_set[j, 6]), numpy.int_(d_set[j, 7]))\n",
    "                if diff > 0:\n",
    "                    team1_pos += 1\n",
    "                else:\n",
    "                    team1_nav += 1\n",
    "            if d_set[j][4] == d_set[i][5]:\n",
    "                diff = numpy.subtract(numpy.int_(d_set[j, 6]), numpy.int_(d_set[j, 7]))\n",
    "                if diff > 0:\n",
    "                    team2_pos += 1\n",
    "                else:\n",
    "                    team2_nav += 1\n",
    "        alpha = (team1_pos - team1_nav) - (team2_pos - team2_nav + home_margin)\n",
    "        # print '-----alpha------', alpha\n",
    "        # print '-----d_set[i][8]------', d_set[i][8]\n",
    "        # print '-----testPredict[k][0]------', testPredict[k][0]\n",
    "        # print '-----x------', d_set[i][8] + testPredict[k][0] - alpha * 0.8\n",
    "        x = sigmoid(0.1, d_set[i][8] + testPredict[k][0] - alpha * 0.8)\n",
    "        win_loss.append(x)\n",
    "        k += 1\n",
    "    return numpy.array(win_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'week', 'date', 'time', 'team', 'opponent_team', 'points', 'opponent_points', 'points_difference', 'spread', 'spread_percentage_sbr', 'spread_percentage_sportsplays', 'spread_percentage_vegas']\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv('seeds/inputs.csv')\n",
    "c = df.columns\n",
    "print([c[i] for i in [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 14, 19, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type the date for which you would like to get (for example: 20160210):20170301\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(20)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('seeds/inputs.csv', usecols = [1, 2, 3, 4, 7, 8, 9, 10, 11, 12, 14, 19, 20, 21], engine = 'python')\n",
    "dataset = dataframe.values\n",
    "\n",
    "select_date = input (\"Please type the date for which you would like to get (for example: 20160210):\")\n",
    "select_date = int (select_date)\n",
    "\n",
    "for i in range(0, len(dataset[:, 4])):\n",
    "    if dataset[:, 4][i] not in teams:\n",
    "        teams.append(dataset[:, 4][i])\n",
    "len_teamName = len(teams)\n",
    "\n",
    "original_dataset = dataset\n",
    "dataset, teamnames = normalize_team(dataset)\n",
    "dataset_clone = dataset\n",
    "\n",
    "# get maximum of point difference\n",
    "point_diff_max = numpy.amax(dataset[:, 4])\n",
    "point_diff_min = numpy.amin(dataset[:, 4])\n",
    "\n",
    "# get maximum of spread\n",
    "spread_max = numpy.amax(dataset[:, 5])\n",
    "spread_min = numpy.amin(dataset[:, 5])\n",
    "\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "dataset = numpy.delete(dataset, [1], axis = 1)\n",
    "\n",
    "# create and fit the LSTM network\n",
    "dim_inputs = 700\n",
    "dim_outputs = 1\n",
    "\n",
    "mem_blocks = 2\n",
    "tt_batch_size = 1\n",
    "tp_batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denilv/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1210: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  return cls(**config)\n",
      "/home/denilv/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1210: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(input_dtype=\"float32\", go_backwards=False, batch_input_shape=[None, Non..., return_sequences=False, trainable=True, name=\"lstm_1\", stateful=False, unroll=False, activation=\"tanh\", unit_forget_bias=True, input_shape=(None, 73), units=2, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0.0, recurrent_dropout=0.0, implementation=0)`\n",
      "  return cls(**config)\n",
      "/home/denilv/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1210: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(name=\"dropout_1\", trainable=True, rate=0.2)`\n",
      "  return cls(**config)\n",
      "/home/denilv/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1210: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(trainable=True, input_dim=None, activity_regularizer=None, name=\"dense_1\", activation=\"sigmoid\", units=1, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Optimizer weight shape (1,) not compatible with provided weight shape (2, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2c5f540eaaa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'saveModels/ModelWeeklyWithVPT.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model = Sequential()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.add(LSTM(input_dim = dim_inputs, output_dim = 300, return_sequences = True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# model.add(LSTM(input_dim = 300, output_dim = 500, return_sequences = True))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.add(Dropout(0.2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/denilv/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0moptimizer_weight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0moptimizer_weight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptimizer_weights_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_weight_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_weight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/denilv/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m     77\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                  \u001b[0;34m' not compatible with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                                  'provided weight shape ' + str(w.shape))\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Optimizer weight shape (1,) not compatible with provided weight shape (2, 1)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model = load_model('saveModels/ModelWeeklyWithVPT.h5')\n",
    "# model.add(LSTM(input_dim = dim_inputs, output_dim = 300, return_sequences = True))  \n",
    "# model.add(LSTM(input_dim = 300, output_dim = 500, return_sequences = True))  \n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(input_dim = 500, output_dim = 200, return_sequences = False))  \n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(input_dim = 200, output_dim = dim_outputs))  \n",
    "# model.add(Activation('sigmoid'))\n",
    "# model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# split into train and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainLength, testLength = getLength(dataset_clone, select_date)\n",
    "\n",
    "train, test = dataset[0:trainLength,:], dataset[trainLength:testLength,:]\n",
    "trainX, trainY = create_dataset(train)\n",
    "testX, testY = create_dataset(test)\n",
    "\n",
    "testTeams = teamnames[trainLength:testLength,:]\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "tt_nb_epoch = 200\n",
    "\n",
    "# training NN Model\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0.0000001, patience = 2, verbose = 0, mode = 'max')\n",
    "hist = model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    nb_epoch = tt_nb_epoch,\n",
    "    batch_size = tt_batch_size,\n",
    "    shuffle = True,\n",
    "    show_accuracy = True,\n",
    "    verbose = 1,\n",
    "    validation_split = 0.1,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate the network\n",
    "loss, accuracy = model.evaluate(trainX, trainY)\n",
    "# print \"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy * 100)\n",
    "# print (\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy * 100))\n",
    "\n",
    "# make predictions\n",
    "testPredict = model.predict(testX, batch_size = tp_batch_size, verbose = 1)\n",
    "\n",
    "testPredict = testPredict * (point_diff_max - point_diff_min) + point_diff_min\n",
    "\n",
    "result = [[int(numpy.around(x, decimals = 0))] for x in testPredict]\n",
    "\n",
    "past_win_loss = getConfidence(testTeams, original_dataset, testPredict, select_date)\n",
    "\n",
    "testPredict = numpy.concatenate((numpy.array(testTeams), result), axis = 1)\n",
    "\n",
    "NN_Percent, Consensus_Percent, WinLoss_Percent, winloss_array, Winner_array = getPercentage(testPredict)\n",
    "\n",
    "winloss = [[x] for x in winloss_array]\n",
    "modified_testPredict = numpy.concatenate((testPredict, winloss), axis = 1)\n",
    "\n",
    "winloss1 = [[x] for x in Winner_array]\n",
    "modified_testPredict = numpy.concatenate((modified_testPredict, winloss1), axis = 1)\n",
    "\n",
    "winloss = [[x] for x in winloss_array]\n",
    "modified_testPredict = numpy.concatenate((testPredict, winloss), axis = 1)\n",
    "\n",
    "result1 = [[x] for x in past_win_loss]\n",
    "modified_testPredict = numpy.concatenate((numpy.array(modified_testPredict), result1), axis = 1)\n",
    "\n",
    "print '--------------The prediction Result for date', select_date, '---------------'\n",
    "print modified_testPredict\n",
    "print 'The percentage of Result for date', select_date, ' was ', NN_Percent, '%.'\n",
    "print 'The percentage of Consensus Prediction for date', select_date, ' was ', Consensus_Percent, '%.'\n",
    "\n",
    "# write the prediction result to csv file named result.csv\n",
    "filename = 'results/resultNormalWithVPT-' + str(select_date) + '.csv'\n",
    "with open(filename, \"wb\") as f:\n",
    "    numpy.savetxt(f, testPredict, delimiter=\",\", fmt=\"%s\")\n",
    "\n",
    "# save Model\n",
    "filename = 'saveModels/ModelNormal-' + str(select_date) + '.h5'\n",
    "model.save(filename)\n",
    "print 'The trained Model saved successfully!'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
